{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.tools import numerai_api, utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# setup logger\n",
    "log_fmt = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers[0].stream = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df.loc[(df['data_type'] == 'validation') | \n",
    "                       (df['data_type'] == 'test') | \n",
    "                       (df['data_type'] == 'live'), 'feature1':'feature50']\n",
    "\n",
    "df_validation_predict = df.loc[df['data_type'] == 'validation','feature1':'feature50']\n",
    "df_validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "\n",
    "X_train_era = df.loc[df['data_type'] == 'train', :].drop(['data_type','target'], axis=1)\n",
    "y_train_era = df.loc[df['data_type'] == 'train', ['era','target']]\n",
    "\n",
    "X_train = X_train_era.drop('era', axis=1)\n",
    "y_train = y_train_era['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is done with cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for i in range(0,10):\n",
    "    \n",
    "    log_reg_model.fit(X_split_train[i].drop('era', axis=1), y_split_train[i]['target'])\n",
    "    score = log_reg_model.score(X_split_test[i].drop('era', axis=1), y_split_test[i]['target'])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "#cv_log_loss = cross_val_score(lg, X_train.drop('era', axis=1), y_train['target'], cv=cv, scoring='neg_log_loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(df_predict_feat, model, filename='predictions.csv', filter_=np.empty(0)):\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = df_predict_feat.index\n",
    "    \n",
    "    if filter_.any():\n",
    "        df_predict_feat = df_predict_feat.drop(df_predict_feat.columns[filter_], axis=1)\n",
    "    submission['probability'] = model.predict_proba(df_predict_feat)[:,1]\n",
    "    submission.to_csv(filename,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_log_loss(model, df, filter_=np.empty(0)):\n",
    "    df_validation_predict = df.loc[df['data_type'] == 'validation','feature1':'feature50']\n",
    "    if filter_.any():\n",
    "        df_validation_predict = df_validation_predict.drop(df_validation_predict.columns[filter_], axis=1)\n",
    "    \n",
    "    df_validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "    validation_prediction = model.predict_proba(df_validation_predict)\n",
    "    return log_loss(df_validation_target, validation_prediction)\n",
    "\n",
    "def check_consistency(model, df, filter_=np.empty(0)):\n",
    "    eras_passed=0\n",
    "    for era in df.loc[df['data_type']=='validation',:].era.unique():\n",
    "        loss = get_validation_log_loss(model,df.loc[df['era']==era,:],filter_)\n",
    "        if loss < 0.693:\n",
    "            eras_passed+=1\n",
    "\n",
    "    return eras_passed/12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-31 16:14:41,187 - INFO - Fitting model\n",
      "2018-01-31 16:14:54,863 - INFO - Model fit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.692945879397903"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_baseline = LogisticRegression(random_state=21)\n",
    "logger.info('Fitting model')\n",
    "lg_baseline.fit(X_train, y_train)\n",
    "logger.info('Model fit')\n",
    "\n",
    "create_submission(df_prediction, lg_baseline, 'predictions_baseline.csv')\n",
    "baseline_logloss = get_validation_log_loss(lg_baseline, df)\n",
    "baseline_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-31 16:16:22,163 - INFO - Fitting model\n",
      "2018-01-31 16:16:33,231 - INFO - Model fit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6929470574555433"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "validation_features_scaled = scaler.transform(df.loc[df['data_type']=='validation','feature1':'feature50'])\n",
    "\n",
    "lg_scaled = LogisticRegression(n_jobs=-1, solver='saga', random_state=21)\n",
    "logger.info('Fitting model')\n",
    "lg_scaled.fit(X_train_scaled, y_train)\n",
    "logger.info('Model fit')\n",
    "\n",
    "validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "validation_prediction = lg_scaled.predict_proba(validation_features_scaled)\n",
    "lg_scaled_logloss = log_loss(validation_target, validation_prediction)\n",
    "lg_scaled_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_important_features = np.where(abs(lg_baseline.coef_) > .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.692945879397903 baseline \n",
    "0.6929555810479051 standard scaled \n",
    "0.6929555168509783 robust scaled\n",
    "0.6929470574555433 max abs scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-31 15:39:51,440 - INFO - Fitting model\n"
     ]
    }
   ],
   "source": [
    "lg_opt = LogisticRegression(n_jobs=-1, random_state=21, \n",
    "                            penalty='l1')\n",
    "logger.info('Fitting model')\n",
    "lg_opt.fit(X_train, y_train)\n",
    "logger.info('Model fit')\n",
    "\n",
    "lg_opt_logloss = get_validation_log_loss(lg_opt, df)\n",
    "lg_opt_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-28 11:03:17,990 - INFO - Starting model fit\n",
      "2018-01-28 11:06:18,937 - INFO - Model fitted\n"
     ]
    }
   ],
   "source": [
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "lg_group = LogisticRegressionCV(cv=cv)\n",
    "logger.info('Starting model fit')\n",
    "lg_group.fit(X_train, y_train)\n",
    "logger.info('Model fitted')\n",
    "group_logloss = get_validation_log_loss(lg_group, df)\n",
    "group_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05782708551779613"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(group_logloss-baseline_logloss)/group_logloss*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.802949569919889e-06\n",
      "-6.025499421191682e-06\n"
     ]
    }
   ],
   "source": [
    "print(0.6926237830311198 - 0.6926199800815499)\n",
    "print(0.6926199800815499 - 0.6926260055809711)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7809569239067203 default\n",
    "0.6926539978867465 n_estimators=1000,max_leaf_nodes=15\n",
    "0.6926281272932799 n_estimators=500,max_leaf_nodes=15\n",
    "0.6984105280907695 n_estimators=100,max_leaf_nodes=None\n",
    "0.6928226448632782 n_estimators=100,max_leaf_nodes=5\n",
    "\n",
    "0.6926490053669208 n_estimators=50,max_leaf_nodes=15\n",
    "0.6926264881596373 n_estimators=100,max_leaf_nodes=15\n",
    "0.6926237830311198 n_estimators=150,max_leaf_nodes=15\n",
    "0.6926199800815499 n_estimators=175,max_leaf_nodes=15\n",
    "0.6926260055809711 n_estimators=200,max_leaf_nodes=15\n",
    "\n",
    "\n",
    "0.6925970274264839 n_estimators=175,max_leaf_nodes=20\n",
    "0.6925456965520028 n_estimators=175,max_leaf_nodes=30\n",
    "0.6925077743059121 n_estimators=175,max_leaf_nodes=50\n",
    "0.6924796040625568 n_estimators=175,max_leaf_nodes=100\n",
    "0.6924269351737405 n_estimators=175,max_leaf_nodes=150\n",
    "0.6923994850550886 n_estimators=175,max_leaf_nodes=200\n",
    "0.692395593293289  n_estimators=175,max_leaf_nodes=205\n",
    "0.6924060245108058 n_estimators=175,max_leaf_nodes=210\n",
    "0.6924183696339382 n_estimators=175,max_leaf_nodes=225\n",
    "0.6924205980793621 n_estimators=175,max_leaf_nodes=250\n",
    "0.6924420819324049 n_estimators=175,max_leaf_nodes=300\n",
    "0.692472160869273  n_estimators=175,max_leaf_nodes=400\n",
    "\n",
    "\n",
    "0.6924482343001823 n_estimators=175,max_leaf_nodes=205 entropy\n",
    "\n",
    "0.6925234826212867 n_estimators=175,max_leaf_nodes=205,max_features=log2\n",
    "0.692395593293289  n_estimators=175,max_leaf_nodes=205\n",
    "0.6924785583536479 n_estimators=175,max_leaf_nodes=205,max_features=8\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10\n",
    "0.6924414990011737 n_estimators=175,max_leaf_nodes=205,max_features=11\n",
    "0.692452599084798  n_estimators=175,max_leaf_nodes=205,max_features=12\n",
    "0.692434921786358  n_estimators=175,max_leaf_nodes=205,max_features=15\n",
    "0.6924986626987714 n_estimators=175,max_leaf_nodes=205,max_features=20\n",
    "0.6925193101183662 n_estimators=175,max_leaf_nodes=205,max_features=30\n",
    "0.6924359465554766 n_estimators=175,max_leaf_nodes=205,max_features=50\n",
    "\n",
    "0.6924264481698214 n_estimators=175,max_leaf_nodes=205,max_features=10, bootstrap=False\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10, oob_score=True\n",
    "\n",
    "0.6924813705374833 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=5\n",
    "0.6924325504443605 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=10\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=50\n",
    "0.6923830374594487 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=30\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=40\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=35\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=32\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=31\n",
    "\n",
    "0.6924227331958277 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=0.01\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=3\n",
    "0.6924284376896439 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=50\n",
    "0.6923971988691803 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=20\n",
    "0.6924490732428077 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=10\n",
    "\n",
    "0.6922934463211274 top 15 max 10\n",
    "0.6930808753582083 top 4\n",
    "0.69265250301624   top 7 no max features\n",
    "0.6924978160188072 top 11\n",
    "0.6923629534794612 top 22 max 10\n",
    "0.6923870823834799 top 22 no max features\n",
    "0.6923336618872348 top 19 max 10\n",
    "0.6923747980103881 top 17 max 10\n",
    "0.6923650167342608 top 16 max 10\n",
    "0.6923704226959646 top 14\n",
    "\n",
    "0.6922934463211274 top 15 max 10\n",
    "0.6922937527006087 max abs scaler\n",
    "0.6922938995815806 robust scaler\n",
    "0.692294168860375  standard scaler\n",
    "0.6922932509521263 standard no mean\n",
    "\n",
    "0.6922837586171385\n",
    "\n",
    "0.6924052073894894 PCA reduction to 7\n",
    "0.6923661805741801 PCA reduction to \n",
    "0.6923270568188112\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-03 20:29:04,329 - INFO - Starting model fit\n",
      "2018-02-03 20:31:19,644 - INFO - Model fitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6924603132117868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_baseline = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=170, \n",
    "                             max_leaf_nodes=203,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc_baseline.fit(X_train, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n",
    "rfc_baseline_logloss = get_validation_log_loss(rfc_baseline, df)\n",
    "rfc_baseline_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc_baseline, open('rfc_baseline_93.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA\n",
      " Volume Serial Number is 6E14-EF9D\n",
      "\n",
      " Directory of D:\\Projects\\numerai\\learning_numerai\\notebooks\n",
      "\n",
      "31/01/2018  06:10    <DIR>          .\n",
      "31/01/2018  06:10    <DIR>          ..\n",
      "28/01/2018  09:46                 0 .gitkeep\n",
      "29/01/2018  10:36    <DIR>          .ipynb_checkpoints\n",
      "31/01/2018  06:08            38,073 Data processing testing.ipynb\n",
      "31/01/2018  03:30            21,369 Numerai API.ipynb\n",
      "31/01/2018  04:14         9,038,574 predictions_baseline.csv\n",
      "29/01/2018  08:52         9,056,479 predictions_RF.csv\n",
      "30/01/2018  06:12         9,058,033 predictions_RF_top15_CV.csv\n",
      "29/01/2018  08:55         9,057,467 predictions_RF_top15_rand.csv\n",
      "31/01/2018  06:10         5,030,908 rfc_baseline.pkl\n",
      "               8 File(s)     41,300,903 bytes\n",
      "               3 Dir(s)  298,262,204,416 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['feature1', 'feature2', 'feature6', 'feature9', 'feature11',\n",
       "       'feature15', 'feature17', 'feature25', 'feature28', 'feature29',\n",
       "       'feature31', 'feature34', 'feature36', 'feature41', 'feature42',\n",
       "       'feature46'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_baseline = pickle.load(open('rfc_baseline_93.pkl','rb'))\n",
    "filter_ = np.where(rfc_baseline.feature_importances_ < 0.020)[0]\n",
    "X_train_important = X_train.drop(X_train.columns[filter_], axis=1)\n",
    "print(len(X_train_important.columns))\n",
    "X_train_important.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-03 20:31:20,159 - INFO - Starting model fit\n",
      "2018-02-03 20:33:29,095 - INFO - Model fitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6924185377721743"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_top15 = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=170, \n",
    "                             max_leaf_nodes=203,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc_top15.fit(X_train_important, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n",
    "rfc_logloss = get_validation_log_loss(rfc_top15, df, filter_)\n",
    "rfc_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc_top15, open('rfc_top_93.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "scaler.fit_transform(X_train_important)\n",
    "X_train_scaled = scaler.transform(X_train_important)\n",
    "validation_features = df.loc[df['data_type']=='validation','feature1':'feature50']\n",
    "validation_features = validation_features.drop(validation_features.columns[filter_], axis=1)\n",
    "validation_features_scaled = scaler.transform(validation_features)\n",
    "\n",
    "pca = PCA(n_components=15, random_state=21)\n",
    "pca.fit(X_train_scaled)\n",
    "X_pca = pca.transform(X_train_scaled)\n",
    "val_feat_pca = pca.transform(validation_features_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-03 20:33:30,327 - INFO - Starting model fit\n",
      "2018-02-03 20:37:14,969 - INFO - Model fitted\n"
     ]
    }
   ],
   "source": [
    "rfc_top15_scaled = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=170, \n",
    "                             max_leaf_nodes=203,\n",
    "                             max_features=15,\n",
    "                             random_state=21)\n",
    "\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc_top15_scaled.fit(X_pca, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925541776106379"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "validation_prediction = rfc_top15_scaled.predict_proba(val_feat_pca)\n",
    "rfc_scaled_logloss = log_loss(validation_target, validation_prediction)\n",
    "rfc_scaled_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_consistency(rfc, df, filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_original_submission(df, model, amount, scaler, filename='predictions.csv', filter_=np.empty(0)):\n",
    "    random.seed(21)\n",
    "    submission = df.loc[(df['data_type'] == 'validation') | \n",
    "                       (df['data_type'] == 'test') | \n",
    "                       (df['data_type'] == 'live'), :]\n",
    "    cols = submission.columns.tolist()\n",
    "    cols = cols[2:53] + cols[0:2]\n",
    "    submission = submission[cols]\n",
    "    \n",
    "    df_predict_feat = submission.loc[:,'feature1':'feature50']\n",
    "    if filter_.any():\n",
    "        df_predict_feat = df_predict_feat.drop(df_predict_feat.columns[filter_], axis=1)\n",
    "        \n",
    "    df_predict_feat = scaler.transform(df_predict_feat)        \n",
    "    submission['probability'] = model.predict_proba(df_predict_feat)[:,1]\n",
    "    submission['probability'] = submission['probability'] + random.uniform(-amount,amount)\n",
    "    \n",
    "    validation_data = submission.loc[submission['data_type'] == 'validation', :]\n",
    "    validation_target = validation_data.loc[:, 'target']\n",
    "    validation_prediction = validation_data.loc[:, 'probability']\n",
    "    validation_log_loss = log_loss(validation_target, validation_prediction)\n",
    "    print(\"Logloss: {}\".format(validation_log_loss))\n",
    "    \n",
    "    eras_passed=0\n",
    "    for era in validation_data['era'].unique():\n",
    "        era_data = validation_data.loc[validation_data['era']==era,:]\n",
    "        era_target = era_data.loc[:, 'target']\n",
    "        era_prediction = era_data.loc[:, 'probability']\n",
    "        era_log_loss = log_loss(era_target, era_prediction)\n",
    "        if era_log_loss < 0.693:\n",
    "            eras_passed+=1\n",
    "\n",
    "    print(\"Consistency: {}\".format(eras_passed/12))\n",
    "    submission['id'] = submission.index\n",
    "    submission = submission.loc[:, ['id','probability']]\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 0.6924171968616661\n",
      "Consistency: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "create_original_submission(df, rfc_top15, 0.012, 'predictions_RF_top15_rand.csv', filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "scaler.fit_transform(X_train_important)\n",
    "X_train_scaled = scaler.transform(X_train_important)\n",
    "validation_features = df.loc[df['data_type']=='validation','feature1':'feature50']\n",
    "validation_features = validation_features.drop(validation_features.columns[filter_], axis=1)\n",
    "validation_features_scaled = scaler.transform(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-04 12:12:58,705 - INFO - Starting model fit\n",
      "2018-02-04 15:06:08,423 - INFO - Model fitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_features': 10, 'max_leaf_nodes': 206, 'n_estimators': 170}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=10)\n",
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [168, 170, 172,],\n",
    "    'max_leaf_nodes': [204, 206, 210,],\n",
    "    'max_features': [10,],\n",
    "}\n",
    "\n",
    "rfc_top15_scaled_CV = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=170, \n",
    "                             max_leaf_nodes=203,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "\n",
    "rfc_top15_scaled_CV = GridSearchCV(n_jobs=-1, estimator=rfc_top15_scaled_CV, param_grid=param_grid, cv=cv)\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc_top15_scaled_CV.fit(X_train_scaled, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n",
    "rfc_top15_scaled_CV.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_top15_scaled_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924258008731194"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "validation_prediction = rfc_top15_scaled_CV.predict_proba(validation_features_scaled)\n",
    "rfc_top15_scaled_CV_logloss = log_loss(validation_target, validation_prediction)\n",
    "rfc_top15_scaled_CV_logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6924258008731194 features 10 nodes 206 estimators 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 0.6923692745555485\n",
      "Consistency: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "create_original_submission(df, rfc_top15_scaled_CV, 0.01, scaler, 'predictions_RF_top15_CV.csv', filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
