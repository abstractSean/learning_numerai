{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.tools import numerai_api\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# setup logger\n",
    "log_fmt = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers[0].stream = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round_number = numerai_api.get_current_round()\n",
    "round_number = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(os.path.pardir,'data','raw')\n",
    "raw_data_file = os.path.join(raw_data_path, '{}_numerai_raw.pkl'.format(round_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_pickle(raw_data_file)\n",
    "except FileNotFoundError:\n",
    "    get_data = os.path.join(os.path.pardir, 'src', 'data', 'get_raw_data_binary.py')\n",
    "    !python $get_data\n",
    "    df = pd.read_pickle(raw_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df.loc[(df['data_type'] == 'validation') | \n",
    "                       (df['data_type'] == 'test') | \n",
    "                       (df['data_type'] == 'live'), 'feature1':'feature50']\n",
    "\n",
    "df_validation_predict = df.loc[df['data_type'] == 'validation','feature1':'feature50']\n",
    "df_validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "\n",
    "X_train_era = df.loc[df['data_type'] == 'train', :].drop(['data_type','target'], axis=1)\n",
    "y_train_era = df.loc[df['data_type'] == 'train', ['era','target']]\n",
    "\n",
    "X_train = X_train_era.drop('era', axis=1)\n",
    "y_train = y_train_era['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is done with cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for i in range(0,10):\n",
    "    \n",
    "    log_reg_model.fit(X_split_train[i].drop('era', axis=1), y_split_train[i]['target'])\n",
    "    score = log_reg_model.score(X_split_test[i].drop('era', axis=1), y_split_test[i]['target'])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gkf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-97bd9fad6a4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'era'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#cv_log_loss = cross_val_score(lg, X_train.drop('era', axis=1), y_train['target'], cv=cv, scoring='neg_log_loss')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gkf' is not defined"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "#cv_log_loss = cross_val_score(lg, X_train.drop('era', axis=1), y_train['target'], cv=cv, scoring='neg_log_loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(df_predict_feat, model, filename='predictions.csv', filter_=np.empty(0)):\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = df_predict_feat.index\n",
    "    \n",
    "    if filter_.any():\n",
    "        df_predict_feat = df_predict_feat.drop(df_predict_feat.columns[filter_], axis=1)\n",
    "    submission['probability'] = model.predict_proba(df_predict_feat)[:,1]\n",
    "    submission.to_csv(filename,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_log_loss(model, df, filter_=np.empty(0)):\n",
    "    df_validation_predict = df.loc[df['data_type'] == 'validation','feature1':'feature50']\n",
    "    if filter_.any():\n",
    "        df_validation_predict = df_validation_predict.drop(df_validation_predict.columns[filter_], axis=1)\n",
    "    \n",
    "    df_validation_target = df.loc[df['data_type'] == 'validation','target']\n",
    "    validation_prediction = model.predict_proba(df_validation_predict)\n",
    "    return log_loss(df_validation_target, validation_prediction)\n",
    "\n",
    "def check_consistency(model, df, filter_=np.empty(0)):\n",
    "    eras_passed=0\n",
    "    for era in df.loc[df['data_type']=='validation',:].era.unique():\n",
    "        loss = get_validation_log_loss(model,df.loc[df['era']==era,:],filter_)\n",
    "        if loss < 0.693:\n",
    "            eras_passed+=1\n",
    "\n",
    "    return eras_passed/12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-28 11:03:01,172 - INFO - Fitting model\n",
      "2018-01-28 11:03:17,190 - INFO - Model fit\n",
      "2018-01-28 11:03:17,191 - INFO - Creating submission\n",
      "2018-01-28 11:03:17,849 - INFO - Submission created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nsubmission = pd.DataFrame()\\nsubmission['id'] = df_prediction.index\\nsubmission['probability'] = lg_baseline.predict_proba(df_prediction)[:,1]\\nsubmission.to_csv('predictions_baseline.csv',index=None)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_baseline = LogisticRegression()\n",
    "logger.info('Fitting model')\n",
    "lg_baseline.fit(X_train, y_train)\n",
    "logger.info('Model fit')\n",
    "\n",
    "create_submission(df_prediction, lg_baseline, 'predictions_baseline.csv')\n",
    "baseline_logloss = get_validation_log_loss(lg_baseline, df)\n",
    "baseline_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-28 11:03:17,990 - INFO - Starting model fit\n",
      "2018-01-28 11:06:18,937 - INFO - Model fitted\n"
     ]
    }
   ],
   "source": [
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "lg_group = LogisticRegressionCV(cv=cv)\n",
    "logger.info('Starting model fit')\n",
    "lg_group.fit(X_train, y_train)\n",
    "logger.info('Model fitted')\n",
    "group_logloss = get_validation_log_loss(lg_group, df)\n",
    "group_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05782708551779613"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(group_logloss-baseline_logloss)/group_logloss*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.802949569919889e-06\n",
      "-6.025499421191682e-06\n"
     ]
    }
   ],
   "source": [
    "print(0.6926237830311198 - 0.6926199800815499)\n",
    "print(0.6926199800815499 - 0.6926260055809711)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7809569239067203 default\n",
    "0.6926539978867465 n_estimators=1000,max_leaf_nodes=15\n",
    "0.6926281272932799 n_estimators=500,max_leaf_nodes=15\n",
    "0.6984105280907695 n_estimators=100,max_leaf_nodes=None\n",
    "0.6928226448632782 n_estimators=100,max_leaf_nodes=5\n",
    "\n",
    "0.6926490053669208 n_estimators=50,max_leaf_nodes=15\n",
    "0.6926264881596373 n_estimators=100,max_leaf_nodes=15\n",
    "0.6926237830311198 n_estimators=150,max_leaf_nodes=15\n",
    "0.6926199800815499 n_estimators=175,max_leaf_nodes=15\n",
    "0.6926260055809711 n_estimators=200,max_leaf_nodes=15\n",
    "\n",
    "\n",
    "0.6925970274264839 n_estimators=175,max_leaf_nodes=20\n",
    "0.6925456965520028 n_estimators=175,max_leaf_nodes=30\n",
    "0.6925077743059121 n_estimators=175,max_leaf_nodes=50\n",
    "0.6924796040625568 n_estimators=175,max_leaf_nodes=100\n",
    "0.6924269351737405 n_estimators=175,max_leaf_nodes=150\n",
    "0.6923994850550886 n_estimators=175,max_leaf_nodes=200\n",
    "0.692395593293289  n_estimators=175,max_leaf_nodes=205\n",
    "0.6924060245108058 n_estimators=175,max_leaf_nodes=210\n",
    "0.6924183696339382 n_estimators=175,max_leaf_nodes=225\n",
    "0.6924205980793621 n_estimators=175,max_leaf_nodes=250\n",
    "0.6924420819324049 n_estimators=175,max_leaf_nodes=300\n",
    "0.692472160869273  n_estimators=175,max_leaf_nodes=400\n",
    "\n",
    "\n",
    "0.6924482343001823 n_estimators=175,max_leaf_nodes=205 entropy\n",
    "\n",
    "0.6925234826212867 n_estimators=175,max_leaf_nodes=205,max_features=log2\n",
    "0.692395593293289  n_estimators=175,max_leaf_nodes=205\n",
    "0.6924785583536479 n_estimators=175,max_leaf_nodes=205,max_features=8\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10\n",
    "0.6924414990011737 n_estimators=175,max_leaf_nodes=205,max_features=11\n",
    "0.692452599084798  n_estimators=175,max_leaf_nodes=205,max_features=12\n",
    "0.692434921786358  n_estimators=175,max_leaf_nodes=205,max_features=15\n",
    "0.6924986626987714 n_estimators=175,max_leaf_nodes=205,max_features=20\n",
    "0.6925193101183662 n_estimators=175,max_leaf_nodes=205,max_features=30\n",
    "0.6924359465554766 n_estimators=175,max_leaf_nodes=205,max_features=50\n",
    "\n",
    "0.6924264481698214 n_estimators=175,max_leaf_nodes=205,max_features=10, bootstrap=False\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10, oob_score=True\n",
    "\n",
    "0.6924813705374833 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=5\n",
    "0.6924325504443605 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=10\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=50\n",
    "0.6923830374594487 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=30\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=40\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=35\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=32\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,max_depth=31\n",
    "\n",
    "0.6924227331958277 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=0.01\n",
    "0.6923817293453017 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=3\n",
    "0.6924284376896439 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=50\n",
    "0.6923971988691803 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=20\n",
    "0.6924490732428077 n_estimators=175,max_leaf_nodes=205,max_features=10,min_samples_split=10\n",
    "\n",
    "0.6922934463211274 top 15 max 10\n",
    "0.6930808753582083 top 4\n",
    "0.69265250301624   top 7 no max features\n",
    "0.6924978160188072 top 11\n",
    "0.6923629534794612 top 22 max 10\n",
    "0.6923870823834799 top 22 no max features\n",
    "0.6923336618872348 top 19 max 10\n",
    "0.6923747980103881 top 17 max 10\n",
    "0.6923650167342608 top 16 max 10\n",
    "0.6923704226959646 top 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-29 23:14:29,632 - INFO - Starting model fit\n",
      "2018-01-29 23:16:42,560 - INFO - Model fitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6923817293453017"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_baseline = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=175, \n",
    "                             max_leaf_nodes=205,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc_baseline.fit(X_train, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n",
    "rfc_baseline_logloss = get_validation_log_loss(rfc_baseline, df)\n",
    "rfc_baseline_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['feature1', 'feature6', 'feature9', 'feature11', 'feature15',\n",
       "       'feature21', 'feature25', 'feature27', 'feature28', 'feature31',\n",
       "       'feature34', 'feature36', 'feature41', 'feature42', 'feature46'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_ = np.where(rfc_baseline.feature_importances_ < 0.020)[0]\n",
    "X_train_important = X_train.drop(X_train.columns[filter_], axis=1)\n",
    "print(len(X_train_important.columns))\n",
    "X_train_important.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_top15 = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=175, \n",
    "                             max_leaf_nodes=205,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "\n",
    "logger.info('Starting model fit')\n",
    "rfc.fit(X_train_important, y_train)\n",
    "logger.info('Model fitted')\n",
    "\n",
    "rfc_logloss = get_validation_log_loss(rfc, df, filter_)\n",
    "rfc_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_consistency(rfc, df, filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_original_submission(df, model, amount, filename='predictions.csv', filter_=np.empty(0)):\n",
    "    random.seed(21)\n",
    "    submission = df.loc[(df['data_type'] == 'validation') | \n",
    "                       (df['data_type'] == 'test') | \n",
    "                       (df['data_type'] == 'live'), :]\n",
    "    cols = submission.columns.tolist()\n",
    "    cols = cols[2:53] + cols[0:2]\n",
    "    submission = submission[cols]\n",
    "    \n",
    "    df_predict_feat = submission.loc[:,'feature1':'feature50']\n",
    "    if filter_.any():\n",
    "        df_predict_feat = df_predict_feat.drop(df_predict_feat.columns[filter_], axis=1)\n",
    "        \n",
    "            \n",
    "    submission['probability'] = model.predict_proba(df_predict_feat)[:,1]\n",
    "    submission['probability'] = submission['probability'] + random.uniform(-amount,amount)\n",
    "    \n",
    "    validation_data = submission.loc[submission['data_type'] == 'validation', :]\n",
    "    validation_target = validation_data.loc[:, 'target']\n",
    "    validation_prediction = validation_data.loc[:, 'probability']\n",
    "    validation_log_loss = log_loss(validation_target, validation_prediction)\n",
    "    print(\"Logloss: {}\".format(validation_log_loss))\n",
    "    \n",
    "    eras_passed=0\n",
    "    for era in validation_data['era'].unique():\n",
    "        era_data = validation_data.loc[validation_data['era']==era,:]\n",
    "        era_target = era_data.loc[:, 'target']\n",
    "        era_prediction = era_data.loc[:, 'probability']\n",
    "        era_log_loss = log_loss(era_target, era_prediction)\n",
    "        if era_log_loss < 0.693:\n",
    "            eras_passed+=1\n",
    "\n",
    "    print(\"Consistency: {}\".format(eras_passed/12))\n",
    "    submission['id'] = submission.index\n",
    "    submission = submission.loc[:, ['id','probability']]\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 0.6924171968616661\n",
      "Consistency: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "create_original_submission(df, rfc_top15, 0.012, 'predictions_RF_top15_rand.csv', filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-29 23:16:43,106 - INFO - Starting model fit\n",
      "2018-01-30 03:48:52,558 - INFO - Model fitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 203, 'n_estimators': 170}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=10)\n",
    "cv = gkf.split(X_train_era, y_train_era, groups=X_train_era['era'])\n",
    "param_grid = { \n",
    "    'n_estimators': [155, 160, 165, 170, 175],\n",
    "    'max_leaf_nodes': [201, 203, 205]\n",
    "}\n",
    "\n",
    "rfc_CV = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=175, \n",
    "                             max_leaf_nodes=205,\n",
    "                             max_features=10,\n",
    "                             random_state=21)\n",
    "CV_rfc = GridSearchCV(n_jobs=-1, estimator=rfc_CV, param_grid=param_grid, cv=cv)\n",
    "logger.info('Starting model fit')\n",
    "CV_rfc.fit(X_train_important, y_train)\n",
    "logger.info('Model fitted')\n",
    "CV_rfc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 0.6924082459556313\n",
      "Consistency: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "create_original_submission(df, CV_rfc, 0.012, 'predictions_RF_top15_CV.csv', filter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
